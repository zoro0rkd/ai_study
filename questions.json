[
    {
      "type": "objective",
      "number": 1,
      "question": "AI 분야에서 머신러닝과 딥러닝의 주요 차이점은 무엇인가?",
      "choices": [
        "모델 크기",
        "데이터 양",
        "신경망의 유무",
        "학습 속도",
        "정확도"
      ],
      "textboxCount": 0
    },
    {
      "type": "subjective",
      "number": 2,
      "question": "보기에 알맞은 단어는?",
      "explanations": ["A)는 기계 학습 분야의 한 방법으로, 모델이 훈련 중에 본 적 없는 데이터나 클래스에 대해\n예측을 수행할 수 있도록 하는 기술이다. 이 기법은 특히 데이터 레이블링 비용이 많이 들거나,\n실제 환경에서 예측해야 할 클래스가 훈련 데이터에 모두 포함되지 않는 경우에 유용하다.\n최근에는 (A)의 확장된 형태인 Generalized (A)를 활용하는 방법이 주목받고 있다. 이 방법은\n모델이 훈련 중에 본 적 있는 클래스와 본 적 없는 클래스를 동시에 인식할 수 있도록 설계된\n기법이다. Generalized (A)는 더욱 실용적인 시나리오에서의 활용성을 고려하여 개발되었으며,\n실제 환경의 응용에서 더욱 현실적인 문제를 해결하는데 도움을 준다."],
      "textboxCount": 2
    },
    {
      "type": "essay",
      "number": 3,
      "question": "AI 기술 발전이 사회 구조에 미치는 영향에 대해 서술하시오.",
      "choices": [],
      "textboxCount": 1
    },
    {
      "type": "objective",
      "number": 4,
      "question": "다음 설명에 해당하는 AI 기술에 대한 설명으로 맞는 것은",
      "explanations": ["images/Q4.png",
      "ㄱ.",
      "ㄴ.ㅂㅈㄷㄱㄴㅁㅇ펑훚ㅂ대거ㅑㅈ볻ㄴㅇ",
      "ㄷ.ㅁㄴㅇㄹㅁㅇㄹ"],
      "choices": [
        "ㄱ",
        "ㄱ,ㄴ,ㄷ",
        "ㄴ,ㄹ",
        "ㄷ,ㄹ",
        "없음"
      ],
      "textboxCount": 0
    },
    {
      "type": "objective",
      "number": 5,
      "question": "[데이터 증강] 제품 이미지에서 불량을 식별하는 프로젝트에 투입되었다.\n이 팀은 제품 불량 검출을 위한 이미지 분류 모델은 불량 샘플이 전체의 5%에 불과하며, 라벨링 비용이 매우 높다.\n 다음 중 이미지 데이터 증강을 활용하여 모델의 일반화 성능을 높이는 데 가장 적절한 전략은 무엇인가?",
      "choices": [
        "모든 이미지에 Gaussian blur를 강하게 적용하여 증강한다.",
        "불량 샘플만 대상으로 회전, 좌우 반전, 밝기 조절 등의 증강을 수행한다.",
        "정상 제품 이미지를 대상으로만 증강을 수행한다.",
        "증강 없이 기존 데이터만으로 모델을 훈련시킨다.",
        "정답 없음"
      ],
      "textboxCount": 0
    },
  {
    "type": "objective",
    "number": 6,
    "question": "다음 보기에는 데이터 증강 기법과 그에 대한 특징 설명이 짝지어져 있다.\n기법과 설명이 올바르게 매칭된 것을 고르시오.",
    "choices": [
      "Horizontal Flip – 객체 경계를 임의로 잘라 재조합해 과적합을 줄인다.",
      "Random Crop – 이미지 전역의 밝기·색조를 무작위로 조절해 다양한 조명 조건을 학습한다.",
      "CutMix – 두 이미지를 부분적으로 섞어 레이블도 혼합하되, 주요 객체 경계는 일부 유지한다.",
      "Gaussian Blur – 좌우 반전을 통해 시야각 다양성을 확보한다.",
      "Color Jitter – 2-D 가우시안 커널로 전체 이미지를 부드럽게 만들어 노이즈를 제거한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 7,
    "question": "다음 중 데이터 증강에 활용될 수 있는 확산 모델(Diffusion Model)의 Forward Process에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "원본 데이터에 단계적으로 가우시안 노이즈를 추가해 완전히 노이즈화된 상태로 변환한다.",
      "각 단계에서 추가되는 노이즈의 분포(β-schedule)를 조절함으로써 학습 안정성과 생성 품질을 조정할 수 있다.",
      "Forward Process는 사전에 학습된 모델 가중치를 이용해 노이즈를 제거하며, 역전파로 미분 가능하다.",
      "Forward Process가 끝난 후, 역과정(Backward Process)에서 사전 학습된 네트워크가 노이즈를 제거하며 샘플을 복원한다.",
      "Forward Process 동안 데이터는 점차 정보가 사라지므로, 최종 단계에서 거의 순수 가우시안 분포에 수렴한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 8,
    "question": "다음 중 Supervised Learning, Self-Supervised Learning, Unsupervised Learning을 구분한 설명으로 옳은 것을 고르시오.",
    "choices": [
      "Supervised Learning은 레이블이 없는 입력만으로 패턴을 발견하며, K-means 클러스터링이 대표적이다.",
      "Self-Supervised Learning은 원본 데이터에서 레이블을 자동 생성한 후 그 레이블을 예측하도록 학습한다.",
      "Unsupervised Learning은 입력–출력 쌍을 이용해 지도 신호를 직접 최소화하며, 분류(Classification)가 주된 응용이다.",
      "Self-Supervised Learning과 Supervised Learning은 모두 외부 사람이 부여한 정답 라벨을 필요로 한다는 점에서 동일하다.",
      "Supervised Learning은 데이터의 구조나 군집을 자율적으로 탐색하는 데 초점을 두며, 오토인코더(AutoEncoder)가 대표 사례이다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 9,
    "question": "다음 설명에 해당하는 학습 기법의 종류에 해당하는지 않는 것을 고르시오.",
    "explanations": ["이 학습 방식은 레이블이 없는 데이터로부터 학습하는 인공지능 기법이다.\n 이 방법은 레이블이나 주석이 필요하지 않은 대신, 데이터 자체에서 학습 과제를 생성한다. 예를 들어, 이미지에서 일부를 가리고 모델이 가려진 부분을 예측하게 하는 것이 이 학습의 한 예시이다. \n이러한 접근 방식은 모델이 데이터의 내재된 구조와 패턴을 이해하도록 돕는다."
    ],
    "choices": [
      "SimCLR",
      "BYOL",
      "MAE",
      "RotNet",
      "AutoEncoder"
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 10,
    "question": "다음 중 자연어 처리 주요 언어 모델에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "BERT는 양방향 Encoder를 사용하며, 마스킹된 토큰을 예측하는 MLM(Masked Language Modeling)과 두 문장의 연속 관계를 맞히는 NSP(Next Sentence Prediction) 과제를 동시에 학습한다.",
      "GPT-2는 단방향 Decoder 구조로, 다음 토큰 예측(Auto-Regressive Language Modeling)을 통해 사전학습된다.",
      "RoBERTa는 원래 BERT의 NSP 과제를 삭제하고, 동적 마스킹·대용량 배치 등으로 학습 효율을 높였다.",
      "ELECTRA는 Generator가 마스킹 토큰을 예측한 “가짜” 문장 전체를 생성하고, Discriminator가 문장 단위로 진짜/가짜를 판별하는 GAN 구조를 따른다.",
      "T5 (Text-to-Text Transfer Transformer)는 입력과 출력을 모두 텍스트 시퀀스로 취급하여, 번역·요약·질의응답 등 다양한 작업을 “text-to-text” 형태로 통일한다."
    ],
    "textboxCount": 0
  },
    {
      "type": "objective",
      "number": 21,
      "question": "다음은 모델 서빙(Deployment) 시 Dependency Management(의존성 관리)에 관한 설명이다. 옳은 것만을 모두 고른 것은?",
      "explanations": ["ㄱ. 모델 예측이 올바르게 나오려면 코드·모델 가중치·라이브러리 버전이 배포 환경과 개발 환경에서 일치해야 하며, TensorFlow 버전이 하나만 달라도 결과가 달라질 수 있다.",
      "ㄴ. 모델을 ONNX 포맷으로 내보내면 실행 언어나 플랫폼이 달라도 모델-관련 라이브러리 의존성을 줄일 수 있으나, feature preprocessing 코드는 포함되지 않으므로 별도 관리가 필요하다.",
      "ㄷ. Docker 컨테이너는 가상머신(VM)보다 운영체제 전체를 포함하므로 훨씬 무겁다.",
      "ㄹ. 서버리스(AWS Lambda 등)를 사용하면 클라우드가 라이브러리 버전을 자동으로 고정하므로 의존성 충돌을 걱정할 필요가 없다."],
      "choices": ["ㄱ", "ㄴ","ㄱ,ㄴ","ㄴ,ㄷ","ㄱ,ㄴ,ㄹ"],
      "textboxCount": 0
    },
    {
      "type": "objective",
      "number": 22,
      "question": "다음은 모델 서빙(Deployment) 시 Dependency Management(의존성 관리)에 관한 설명이다. 옳은 것만을 모두 고른 것은?",
      "explanations": ["ㄱ. 새 모델을 전 트래픽에 곧바로 배포하기보다 일부 트래픽부터 점진적으로 보내며 검증할 수 있어야 한다.",
      "ㄴ. 배포 후 성능 저하가 발견되면 이전 버전으로 즉시 롤백할 수 있어야 한다.",
      "ㄷ. 버전별로 트래픽을 나누어 동시에 실험(A/B 테스트) 할 수 있어야 한다.",
      "ㄹ. 모델 Rollouts는 단일 모델에만 적용되며, 여러 모델이 연결된 파이프라인 전체에는 적용할 수 없다."],
      "choices": ["ㄱ", "ㄴ","ㄱ,ㄴ","ㄱ,ㄴ,ㄷ","ㄱ,ㄴ,ㄹ"],
      "textboxCount": 0
    },
    {
      "type": "subjective",
      "number": 23,
      "question": "AI 서비스를 운영할 때 모델 캐싱, 데이터 캐싱, 결과 캐싱을 적절히 사용하면 응답 지연과 비용을 크게 줄일 수 있다. 아래 설명 각각에 대해 옳은 내용은 O, 옳지 않은 내용은 X를 쓰시오. (답안 작성 예: O,X,O,X)",
      "explanations": [" 모델 캐싱은 자주 호출되는 모델 가중치를 GPU 메모리에 상주시켜 cold-start latency를 줄이는 대표적 최적화 방법이다. (O/X)",
      "ㄴ. 데이터 캐싱은 특징 전처리 파이프라인이 바뀌더라도 한 번 만들어 두면 재학습 시 그대로 재사용할 수 있으므로, 파이프라인 버전 관리가 필요 없다. (O/X)",
      "ㄷ. 결과 캐싱은 예측 호출 시 모델 버전·입력·TTL 등을 키로 삼아야 하며, 모델이 재배포되면 캐시를 무효화(invalidate)해야 한다. (O/X)",
      "ㄹ. 모델 캐싱·데이터 캐싱·결과 캐싱 모두 RAM보다 디스크 I/O 병목이 적게 발생하므로 SSD보다 HDD 스토리지를 사용하는 것이 일반적이다. (O/X)"],
      "choices": [],
      "textboxCount": 4
    },
    {
      "type": "subjective",
      "number": 24,
      "question": "다음 <보기>에서 (A)에 들어갈 알맞은 단어를 영어로 작성하시오.",
      "explanations": ["(A)는 대형 언어 모델이나 강화학습 에이전트의 행동·출력을 개선하기 위해 사람의 피드백(선호도, 보상, 순위 등)을 활용해 정책을 학습하는 방법으로, 전통적인 강화학습이 환경으로부터 수식화된 보상을 받는 것과 달리 (A)는 사람이 바람직한 응답이나 행동을 선택·평가해 보상 모델을 구축하고 이를 기반으로 정책을 미세 조정(fine-tuning)함으로써 모델이 인간 가치와 안전성을 반영하면서 대량의 수작업 레이블 없이도 고품질 언어·행동을 생성하도록 돕는다."],
      "choices": [],
      "textboxCount": 1
    },
    {
      "type": "subjective",
      "number": 25,
      "question": "다음 <보기>에서 (A)에 들어갈 기법을 영어로 쓰시오.",
      "explanations": ["(A)는 대규모 텍스트-이미지 쌍을 이용해 언어와 시각 표현을 동일 임베딩 공간에 정렬하도록 학습하는 방법으로, 하나의 대형 변환기(Transformer) 아키텍처가 이미지와 캡션을 대조적 손실(contrastive loss) 으로 공동 훈련된다. 이 방식을 거치면 모델은 “설명에 가장 잘 맞는 그림”과 “그림에 가장 잘 맞는 설명”을 동시에 구분하도록 최적화되며, 별도의 태스크-특화 파인튜닝 없이도 이미지 분류·검색·텍스트 기반 이미지 이해 등 다양한 작업에서 제로-샷(zero-shot) 성능을 보여 준다. 또한 사람의 라벨을 추가로 달지 않아도 웹에서 수집한 대규모 데이터를 그대로 활용할 수 있어 효율적이며, 하나의 문장만 바꿔 입력해도 즉시 새로운 클래스를 인식할 수 있는 높은 확장성을 지닌다.",
      "images/Q25.png"],
      "choices": [],
      "textboxCount": 1
    }
  ]