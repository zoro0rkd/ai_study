[
    {
      "type": "objective",
      "number": 1,
      "question": "모델 학습에 사용할 데이터를 수집하는 과정에서, 데이터의 대표성과 공정성을 확보하기 위해 다양한 인구 집단으로부터 데이터를 수집하고, 소수 그룹의 데이터를 추가로 확보하는 등의 과정을 무엇이라고 하며, 이러한 방식은 AI 시스템이 특정 집단에 대해 편향된 판단을 내리는 것을 방지하기 위한 무엇 확보의 핵심적인 요소인가? 다음 중 (A)와 (B)에 들어갈 가장 적절한 용어의 조합을 고르시오. ",
      "choices": [
        "(A) 데이터 증강    (B) 일반화",
        "(A) 데이터 편향 완화  (B) 공정성",
        "(A) 레이블링 자동화  (B) 신뢰도",
        "(A) 특징 엔지니어링  (B) 정확도",
        "(A) 이상치 제거    (B) 안전성"
      ],
      "textboxCount": 0
    },
    {
      "type": "objective",
      "number": 2,
      "question": "모델 학습을 위한 데이터 정제 과정은 데이터 품질을 높이고 모델 성능을 향상시키기 위해 반드시 수행되어야 한다. 다음 중 데이터 정제 단계에서 수행하는 작업에 대한 설명으로 옳지 않은 것은 무엇인가?",
      "explanations": ["결측치가 존재하는 행을 삭제하거나 평균값 등으로 대체할 수 있다.",
    "이상치(Outlier)는 도메인 지식이나 통계 기준에 따라 제거 또는 수정될 수 있다.",
    "데이터 정제 시 중복 데이터를 허용하여 모델의 학습 데이터를 풍부하게 만든다.",
    "문자열로 구성된 범주형 데이터는 전처리 과정에서 Label Encoding 등을 통해 정수형으로 변환될 수 있다.",
    "정규화(Normalization)는 값의 범위를 일정하게 맞춰 모델 학습의 효율성을 높이는 데 도움을 준다."],
      "textboxCount": 0
    },
    {
      "type": "subjective",
      "number": 3,
      "question": "AI 모델의 성능을 높이기 위해 데이터 정제 과정에서 이상치(Outlier)를 처리하는 방법 중 하나는, 수치형 데이터의 분포를 고려하여 ( A ) 기준을 벗어나는 값을 제거하거나 수정하는 것이다. 예를 들어, 표준편차 기반 이상치 제거 방법에서는 평균으로부터 **( B )**배 이상 떨어진 값을 이상치로 간주할 수 있다.",
      "choices": [],
      "textboxCount": 2
    },
    {
      "type": "objective",
      "number": 4,
      "question": "다음 보기 중 데이터 정제 과정에서 사용하는 대표적인 기법에 대한 설명으로 옳은 것만을 모두 고른 조합은 무엇인가?",
      "explanations": ["ㄱ. 결측값(NaN)은 평균이나 최빈값 등으로 대체하거나 제거할 수 있다.", "ㄴ. 문자열 데이터는 정규화(normalization)를 통해 0~1 범위로 변환할 수 있다.", "ㄷ. 이상치는 사분위수 범위(IQR)를 활용하여 탐지하고 처리할 수 있다.", "ㄹ. 중복 데이터는 모델 일반화를 위해 의도적으로 추가하는 것이 좋다."],
      "choices": [
        "ㄱ, ㄷ",
        "ㄱ, ㄴ, ㄷ",
        "ㄴ, ㄷ",
        "ㄱ, ㄷ, ㄹ",
        "ㄱ, ㄴ, ㄹ"
      ],
      "textboxCount": 0
    },
    {
      "type": "subjective",
      "number": 5,
      "question": "데이터 증강은 모델의 일반화 성능을 향상시키기 위한 대표적인 전처리 전략 중 하나이다. 이미지 분류 문제에서 사용되는 대표적인 증강 기법 중 하나인 ( A ) 은(는) 두 개의 이미지와 해당 레이블을 선형적으로 혼합하여 새로운 학습 샘플을 생성하는 방식이다. 이 기법은 결정 경계를 부드럽게 만들어 모델이 과적합을 방지하고 더 일반적인 패턴을 학습하도록 유도한다.",
      "choices": [
        ""
      ],
      "textboxCount": 1
    },
  {
    "type": "objective",
    "number": 6,
    "question": "다음 보기에는 데이터 증강 기법과 그에 대한 특징 설명이 짝지어져 있다.\n기법과 설명이 올바르게 매칭된 것을 고르시오.",
    "choices": [
      "Horizontal Flip – 객체 경계를 임의로 잘라 재조합해 과적합을 줄인다.",
      "Random Crop – 이미지 전역의 밝기·색조를 무작위로 조절해 다양한 조명 조건을 학습한다.",
      "CutMix – 두 이미지를 부분적으로 섞어 레이블도 혼합하되, 주요 객체 경계는 일부 유지한다.",
      "Gaussian Blur – 좌우 반전을 통해 시야각 다양성을 확보한다.",
      "Color Jitter – 2-D 가우시안 커널로 전체 이미지를 부드럽게 만들어 노이즈를 제거한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 7,
    "question": "다음 중 데이터 증강에 활용될 수 있는 확산 모델(Diffusion Model)의 Forward Process에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "원본 데이터에 단계적으로 가우시안 노이즈를 추가해 완전히 노이즈화된 상태로 변환한다.",
      "각 단계에서 추가되는 노이즈의 분포(β-schedule)를 조절함으로써 학습 안정성과 생성 품질을 조정할 수 있다.",
      "Forward Process는 사전에 학습된 모델 가중치를 이용해 노이즈를 제거하며, 역전파로 미분 가능하다.",
      "Forward Process가 끝난 후, 역과정(Backward Process)에서 사전 학습된 네트워크가 노이즈를 제거하며 샘플을 복원한다.",
      "Forward Process 동안 데이터는 점차 정보가 사라지므로, 최종 단계에서 거의 순수 가우시안 분포에 수렴한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 8,
    "question": "다음 중 Supervised Learning, Self-Supervised Learning, Unsupervised Learning을 구분한 설명으로 옳은 것을 고르시오.",
    "choices": [
      "Supervised Learning은 레이블이 없는 입력만으로 패턴을 발견하며, K-means 클러스터링이 대표적이다.",
      "Self-Supervised Learning은 원본 데이터에서 레이블을 자동 생성한 후 그 레이블을 예측하도록 학습한다.",
      "Unsupervised Learning은 입력–출력 쌍을 이용해 지도 신호를 직접 최소화하며, 분류(Classification)가 주된 응용이다.",
      "Self-Supervised Learning과 Supervised Learning은 모두 외부 사람이 부여한 정답 라벨을 필요로 한다는 점에서 동일하다.",
      "Supervised Learning은 데이터의 구조나 군집을 자율적으로 탐색하는 데 초점을 두며, 오토인코더(AutoEncoder)가 대표 사례이다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 9,
    "question": "다음 설명에 해당하는 학습 기법의 종류에 해당하는지 않는 것을 고르시오.",
    "explanations": ["이 학습 방식은 레이블이 없는 데이터로부터 학습하는 인공지능 기법이다.\n 이 방법은 레이블이나 주석이 필요하지 않은 대신, 데이터 자체에서 학습 과제를 생성한다. 예를 들어, 이미지에서 일부를 가리고 모델이 가려진 부분을 예측하게 하는 것이 이 학습의 한 예시이다. \n이러한 접근 방식은 모델이 데이터의 내재된 구조와 패턴을 이해하도록 돕는다."
    ],
    "choices": [
      "SimCLR",
      "BYOL",
      "MAE",
      "RotNet",
      "AutoEncoder"
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 10,
    "question": "다음 중 자연어 처리 주요 언어 모델에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "BERT는 양방향 Encoder를 사용하며, 마스킹된 토큰을 예측하는 MLM(Masked Language Modeling)과 두 문장의 연속 관계를 맞히는 NSP(Next Sentence Prediction) 과제를 동시에 학습한다.",
      "GPT-2는 단방향 Decoder 구조로, 다음 토큰 예측(Auto-Regressive Language Modeling)을 통해 사전학습된다.",
      "RoBERTa는 원래 BERT의 NSP 과제를 삭제하고, 동적 마스킹·대용량 배치 등으로 학습 효율을 높였다.",
      "ELECTRA는 Generator가 마스킹 토큰을 예측한 “가짜” 문장 전체를 생성하고, Discriminator가 문장 단위로 진짜/가짜를 판별하는 GAN 구조를 따른다.",
      "T5 (Text-to-Text Transfer Transformer)는 입력과 출력을 모두 텍스트 시퀀스로 취급하여, 번역·요약·질의응답 등 다양한 작업을 “text-to-text” 형태로 통일한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 11,
    "question": "[데이터 -- 수집] 다음 중 AI 모델 학습을 위한 데이터 수집 시 개인정보 보호를 위한 조치로 가장 적절하지 않은 것을 고르시오.",
    "choices": [
        "민감 정보는 수집 전에 익명화하거나 가명처리한다.",
        "데이터 수집 시 사용자 동의를 받는다.",
        "수집된 데이터는 암호화하여 저장한다.",
        "데이터 수집 목적과 무관한 모든 데이터를 포함하여 최대한 많이 확보한다.",
        "데이터 접근 권한을 최소화하여 관리한다."
    ],
    "explanations": [
      "정답 ④ 출제 의도: AI 데이터 수집 시 개인정보 보호 원칙에 대한 이해를 평가"
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 12,
    "question": "[데이터 수집] 다음 중 센서 기반 시계열 데이터를 수집할 때 발생할 수 있는 문제점으로 가장 적절한 것을 고르시오.",
    "choices": [
        "데이터가 항상 균일한 간격으로 수집된다.",
        "센서 오류로 인해 결측값이 발생할 수 있다.",
        "수집된 데이터는 항상 정규 분포를 따른다.",
        "데이터 수집 시 시간 정보는 불필요하다.",
        "센서 간 데이터 동기화는 자동으로 이루어진다."
    ],
    "explanations": [
      "정답 ② 출제 의도: 시계열 데이터 수집 시 발생 가능한 품질 문제"
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 13,
    "question": "[데이터 정제] 다음 중 이상치(Outlier) 처리 방법으로 가장 적절하지 않은 것을 고르시오.",
    "choices": [
        "IQR 기반 필터링을 통해 극단값을 제거한다.",
        "Z-score를 활용하여 통계적으로 이상치를 탐지한다.",
        "이상치를 그대로 유지하여 모델이 학습하도록 한다.",
        "도메인 전문가의 기준에 따라 수동으로 제거한다.",
        "이상치를 대체값으로 치환하여 모델 입력에 활용한다."
    ],
    "explanations": [
      "정답 : 3",
      "IQR 필터링 : 데이터 이상치 탐지하는 방법으로 비정상적 큰 값과 작은 값을 감지 및 제거함",
      "  - Q1 : 하위 25% 지점, Q3 : 상위 75%지점",
      "  - IQR : Q3-Q1로 중간데이터 50%",
      "z-score : (측정값-평균)/표준편차 --> 측정값과 평균사이 거리를 표준편차 단위로 나타낸것",
      "이상치 처리 방법",
      "  - 탐지방법 : 수치적, 시각적",
      "  - 처리방법 : 제거, 대체, 변환, 강건모델 사용",
      "  - 주의사항 : 이상치도 전문가의 소견에 따라 정상일 경우 있음"
    ],
    "textboxCount": 0
  },
    {
    "type": "objective",
    "number": 13.1,
    "question": "[데이터 정제] 다음은 결측값 처리에 대한 코드이다. 빈칸 (A)에 들어갈 가장 적절한 코드를 고르시오.",
    "choices": [
      "dropna()",
      "fillna(df['feature'].mean())",
      "replace(0)",
      "interpolate()",
      "isnull()"
    ],
      "explanations": [
      "정답 : 2",
      "dropna : 빈데이터 삭제 - 데이터가 빠진 컬럼이 있다면 행 삭제, df.dropna(subset = ['columne1', '...']",
      "fillna : 빈데이터 채우기 - 데이터가 빠진 컬럼이 있다면 주어진 값으로 채음. df.fillna( {'columne1':1. 'columne2':2})"
    ],
    "textboxCount": 0
  },
    {
    "type": "objective",
    "number": 13.2,
    "question": "[데이터 정제] 다음 중 텍스트 데이터 정제 과정에서 일반적으로 수행하지 않는 작업을 고르시오.",
    "choices": [
      "불용어(stopwords) 제거",
      "특수문자 제거",
      "토큰화(tokenization)",
      "단어 임베딩 적용",
      "소문자 변환"
    ],
    "explanations": [
      "정답 : 4",
      "전처리 - 불용어 제거, 특수문자 제거, 정규화(어간 추출, 표제어 추출, 소문자 변화, ... ), 토큰화, ...",
      "단어 임베딩 - 단어를 수치 벡터로 표현하는 방식"
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 14,
    "question": "다음 중 자연어 처리 주요 언어 모델에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "BERT는 양방향 Encoder를 사용하며, 마스킹된 토큰을 예측하는 MLM(Masked Language Modeling)과 두 문장의 연속 관계를 맞히는 NSP(Next Sentence Prediction) 과제를 동시에 학습한다.",
      "GPT-2는 단방향 Decoder 구조로, 다음 토큰 예측(Auto-Regressive Language Modeling)을 통해 사전학습된다.",
      "RoBERTa는 원래 BERT의 NSP 과제를 삭제하고, 동적 마스킹·대용량 배치 등으로 학습 효율을 높였다.",
      "ELECTRA는 Generator가 마스킹 토큰을 예측한 “가짜” 문장 전체를 생성하고, Discriminator가 문장 단위로 진짜/가짜를 판별하는 GAN 구조를 따른다.",
      "T5 (Text-to-Text Transfer Transformer)는 입력과 출력을 모두 텍스트 시퀀스로 취급하여, 번역·요약·질의응답 등 다양한 작업을 “text-to-text” 형태로 통일한다."
    ],
    "textboxCount": 0
  },
  {
    "type": "objective",
    "number": 15,
    "question": "다음 중 자연어 처리 주요 언어 모델에 대한 설명으로 옳지 않은 것은 무엇인가?",
    "choices": [
      "BERT는 양방향 Encoder를 사용하며, 마스킹된 토큰을 예측하는 MLM(Masked Language Modeling)과 두 문장의 연속 관계를 맞히는 NSP(Next Sentence Prediction) 과제를 동시에 학습한다.",
      "GPT-2는 단방향 Decoder 구조로, 다음 토큰 예측(Auto-Regressive Language Modeling)을 통해 사전학습된다.",
      "RoBERTa는 원래 BERT의 NSP 과제를 삭제하고, 동적 마스킹·대용량 배치 등으로 학습 효율을 높였다.",
      "ELECTRA는 Generator가 마스킹 토큰을 예측한 “가짜” 문장 전체를 생성하고, Discriminator가 문장 단위로 진짜/가짜를 판별하는 GAN 구조를 따른다.",
      "T5 (Text-to-Text Transfer Transformer)는 입력과 출력을 모두 텍스트 시퀀스로 취급하여, 번역·요약·질의응답 등 다양한 작업을 “text-to-text” 형태로 통일한다."
    ],
    "textboxCount": 0
  },
    {
      "type": "objective",
      "number": 21,
      "question": "다음은 모델 서빙(Deployment) 시 Dependency Management(의존성 관리)에 관한 설명이다. 옳은 것만을 모두 고른 것은?",
      "explanations": ["ㄱ. 모델 예측이 올바르게 나오려면 코드·모델 가중치·라이브러리 버전이 배포 환경과 개발 환경에서 일치해야 하며, TensorFlow 버전이 하나만 달라도 결과가 달라질 수 있다.",
      "ㄴ. 모델을 ONNX 포맷으로 내보내면 실행 언어나 플랫폼이 달라도 모델-관련 라이브러리 의존성을 줄일 수 있으나, feature preprocessing 코드는 포함되지 않으므로 별도 관리가 필요하다.",
      "ㄷ. Docker 컨테이너는 가상머신(VM)보다 운영체제 전체를 포함하므로 훨씬 무겁다.",
      "ㄹ. 서버리스(AWS Lambda 등)를 사용하면 클라우드가 라이브러리 버전을 자동으로 고정하므로 의존성 충돌을 걱정할 필요가 없다."],
      "choices": ["ㄱ", "ㄴ","ㄱ,ㄴ","ㄴ,ㄷ","ㄱ,ㄴ,ㄹ"],
      "textboxCount": 0
    },
    {
      "type": "objective",
      "number": 22,
      "question": "다음은 모델 서비스를 업데이트할 때 활용되는 “Model Rollouts”에 대한 설명이다. 옳은 것만을 모두 고른 것은?",
      "explanations": ["ㄱ. 새 모델을 전 트래픽에 곧바로 배포하기보다 일부 트래픽부터 점진적으로 보내며 검증할 수 있어야 한다.",
      "ㄴ. 배포 후 성능 저하가 발견되면 이전 버전으로 즉시 롤백할 수 있어야 한다.",
      "ㄷ. 버전별로 트래픽을 나누어 동시에 실험(A/B 테스트) 할 수 있어야 한다.",
      "ㄹ. 모델 Rollouts는 단일 모델에만 적용되며, 여러 모델이 연결된 파이프라인 전체에는 적용할 수 없다."],
      "choices": ["ㄱ", "ㄴ","ㄱ,ㄴ","ㄱ,ㄴ,ㄷ","ㄱ,ㄴ,ㄹ"],
      "textboxCount": 0
    },
    {
      "type": "subjective",
      "number": 23,
      "question": "AI 서비스를 운영할 때 모델 캐싱, 데이터 캐싱, 결과 캐싱을 적절히 사용하면 응답 지연과 비용을 크게 줄일 수 있다. 아래 설명 각각에 대해 옳은 내용은 O, 옳지 않은 내용은 X를 쓰시오. (답안 작성 예: O,X,O,X)",
      "explanations": ["ㄱ. 모델 캐싱은 자주 호출되는 모델 가중치를 GPU 메모리에 상주시켜 cold-start latency를 줄이는 대표적 최적화 방법이다. (O/X)",
      "ㄴ. 데이터 캐싱은 특징 전처리 파이프라인이 바뀌더라도 한 번 만들어 두면 재학습 시 그대로 재사용할 수 있으므로, 파이프라인 버전 관리가 필요 없다. (O/X)",
      "ㄷ. 결과 캐싱은 예측 호출 시 모델 버전·입력·TTL 등을 키로 삼아야 하며, 모델이 재배포되면 캐시를 무효화(invalidate)해야 한다. (O/X)",
      "ㄹ. 모델 캐싱·데이터 캐싱·결과 캐싱 모두 RAM보다 디스크 I/O 병목이 적게 발생하므로 SSD보다 HDD 스토리지를 사용하는 것이 일반적이다. (O/X)"],
      "choices": [],
      "textboxCount": 4
    },
    {
      "type": "subjective",
      "number": 24,
      "question": "다음 <보기>에서 (A)에 들어갈 알맞은 단어를 영어로 작성하시오.",
      "explanations": ["(A)는 대형 언어 모델이나 강화학습 에이전트의 행동·출력을 개선하기 위해 사람의 피드백(선호도, 보상, 순위 등)을 활용해 정책을 학습하는 방법으로, 전통적인 강화학습이 환경으로부터 수식화된 보상을 받는 것과 달리 (A)는 사람이 바람직한 응답이나 행동을 선택·평가해 보상 모델을 구축하고 이를 기반으로 정책을 미세 조정(fine-tuning)함으로써 모델이 인간 가치와 안전성을 반영하면서 대량의 수작업 레이블 없이도 고품질 언어·행동을 생성하도록 돕는다."],
      "choices": [],
      "textboxCount": 1
    },
    {
      "type": "subjective",
      "number": 25,
      "question": "다음 <보기>에서 (A)에 들어갈 기법을 영어로 쓰시오.",
      "explanations": ["(A)는 대규모 텍스트-이미지 쌍을 이용해 언어와 시각 표현을 동일 임베딩 공간에 정렬하도록 학습하는 방법으로, 하나의 대형 변환기(Transformer) 아키텍처가 이미지와 캡션을 대조적 손실(contrastive loss) 으로 공동 훈련된다. 이 방식을 거치면 모델은 “설명에 가장 잘 맞는 그림”과 “그림에 가장 잘 맞는 설명”을 동시에 구분하도록 최적화되며, 별도의 태스크-특화 파인튜닝 없이도 이미지 분류·검색·텍스트 기반 이미지 이해 등 다양한 작업에서 제로-샷(zero-shot) 성능을 보여 준다. 또한 사람의 라벨을 추가로 달지 않아도 웹에서 수집한 대규모 데이터를 그대로 활용할 수 있어 효율적이며, 하나의 문장만 바꿔 입력해도 즉시 새로운 클래스를 인식할 수 있는 높은 확장성을 지닌다.",
      "images/Q25.png"],
      "choices": [],
      "textboxCount": 1
    }
  ]